{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1920283f3ca5492c9c73e313f4449faf": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_61729ae1f68d4cd49288729b837838ec",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠧\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠧</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "61729ae1f68d4cd49288729b837838ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749b5050f7314937a612f94aeb8da672": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_cbeac9d65e46452ab244e0d7600674ab",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mConciseness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mD…\u001b[0m\n✨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDone…\u001b[0m\n✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m      \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Conciseness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">D…</span>\n✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Done…</span>\n✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>      \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "cbeac9d65e46452ab244e0d7600674ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/llm-from-prototype-to-production/blob/main/Eval4pptx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands on: LLM as a judge\n",
        "\n",
        "Goal\n",
        "* see how llm-as-a-judge works in principle\n",
        "* introduction to the G-Eval algorithm ([G-Eval on arxive ](https://arxiv.org/abs/2303.16634))\n",
        "* see how the algorithm uses prompts to generate the actual eval prompt\n",
        "* try out the [DeepEval library](https://docs.confident-ai.com/docs/guides-using-custom-llms)\n",
        "\n",
        "\n",
        "Requirements:\n",
        "* OpenAI api key"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SetUp : create an *llm_run* method calling OpenAI Gpt\n",
        "\n",
        "* define a simple **llm_run** method, that calls the gpt model\n",
        "* try out llm_run"
      ],
      "metadata": {
        "id": "HUWl-0x-524k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wnUuUSG68wlB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install openai\n",
        "!pip install deepeval==1.1.1 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "b47b1c57-2839-4235-e7ab-41bb165cace1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.44.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "CPU times: user 85 ms, sys: 9.43 ms, total: 94.4 ms\n",
            "Wall time: 12.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "h2KpOS917BeW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_run(messages):\n",
        "  if type(messages) == str:\n",
        "    messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "\n",
        "  client = OpenAI()\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        "  )\n",
        "  result = completion.choices[0].message.content\n",
        "  return result"
      ],
      "metadata": {
        "id": "jlJxNTGs6xM3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out our model:"
      ],
      "metadata": {
        "id": "8b-8riV2Vf_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_run(\"who are you ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "nf93aT-JVPol",
        "outputId": "490c76c3-7a3f-43f8-cfb6-1a0eae78c809"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model created by OpenAI, designed to assist with providing information, answering questions, and engaging in conversation on a wide range of topics. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-as-a-judge: in principle"
      ],
      "metadata": {
        "id": "z0jYXr3F9pem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output=\"Witing texts is painful, caus im making mitakes.\""
      ],
      "metadata": {
        "id": "Zh74JhHgQOMl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_eval_prompt = f'''\n",
        "You are an expert on english language, grading a students text with scores between 0 and 10.\n",
        "A text written in proper english, in a fluent style, containing no grammatical or syntax errors is graded 10.\n",
        "A text written in a different language or with spelling errors gets a low score.\n",
        "Also give a detailed explanation why the score was chosen.\n",
        "Do not repeat the students text in your explanation.\n",
        "\n",
        "Always answer in the following json format:\n",
        "{{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"some reason\"\n",
        "}}\n",
        "\n",
        "Examples\n",
        "1. Student Text: Pipes are cylindrical conduits used to transport fluids or gases, typically made of materials like metal, plastic, or concrete.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"The text is written in english and does not contain any syntactical or grammatical erros\"\n",
        "  }}\n",
        "2. Student Text: Zwischen Neonlichtern und Straßenlärm träum ich leise von Freiheit.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 2,\n",
        "    \"reason\": \"The text is written in german and not in english.\"\n",
        "  }}\n",
        "\n",
        "Student Text: {llm_output}\n",
        "Answer:\n",
        "'''"
      ],
      "metadata": {
        "id": "1fUQwWQd8W2D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(\"***** Prompt         :\")\n",
        "print(simple_eval_prompt)\n",
        "\n",
        "answer = llm_run(simple_eval_prompt)\n",
        "\n",
        "print(\"***** Answer         :\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "90PhBKcH8Wu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1ae923-4c98-40e9-dbcd-9466f2b1d090"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Prompt         :\n",
            "\n",
            "You are an expert on english language, grading a students text with scores between 0 and 10.\n",
            "A text written in proper english, in a fluent style, containing no grammatical or syntax errors is graded 10.\n",
            "A text written in a different language or with spelling errors gets a low score.\n",
            "Also give a detailed explanation why the score was chosen.\n",
            "Do not repeat the students text in your explanation.\n",
            "\n",
            "Always answer in the following json format:\n",
            "{\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"some reason\"\n",
            "}\n",
            "\n",
            "Examples\n",
            "1. Student Text: Pipes are cylindrical conduits used to transport fluids or gases, typically made of materials like metal, plastic, or concrete.\n",
            "   Answer:\n",
            "   {\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"The text is written in english and does not contain any syntactical or grammatical erros\"\n",
            "  }\n",
            "2. Student Text: Zwischen Neonlichtern und Straßenlärm träum ich leise von Freiheit.\n",
            "   Answer:\n",
            "   {\n",
            "    \"score\": 2,\n",
            "    \"reason\": \"The text is written in german and not in english.\"\n",
            "  }\n",
            "\n",
            "Student Text: Witing texts is painful, caus im making mitakes.\n",
            "Answer:\n",
            "\n",
            "***** Answer         :\n",
            "{\n",
            "    \"score\": 4,\n",
            "    \"reason\": \"The text is written in English but contains several spelling errors ('Witing' instead of 'writing,' 'caus' instead of 'because,' and 'mitakes' instead of 'mistakes'). These errors affect the overall clarity and fluency of the writing.\"\n",
            "}\n",
            "CPU times: user 106 ms, sys: 3.96 ms, total: 109 ms\n",
            "Wall time: 979 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llm-as-a-judge: G-Eval in principal"
      ],
      "metadata": {
        "id": "xtY98fXd6gl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Idea:** given just a criteria use an llm to generate a detailed evaluation prompt.\n",
        "\n",
        "https://arxiv.org/pdf/2303.16634"
      ],
      "metadata": {
        "id": "BwT8zKTlWg05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G-Eval Phase 1: generate evaluation steps based on the criteria"
      ],
      "metadata": {
        "id": "p4swZQuDG5GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria=\"Grade the english grammar and syntax\""
      ],
      "metadata": {
        "id": "fjD209UWH0Kw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "geval_phase1_prompt = f'''Given an evaluation criteria which outlines how you should judge the Actual Output, generate\n",
        "3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output in\n",
        "relation to one another.\n",
        "\n",
        "Evaluation Criteria:\n",
        "{criteria}\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or\n",
        "explanation is needed.\n",
        "Example JSON:\n",
        "{{\n",
        "    \"steps\": <list_of_strings>\n",
        "}}\n",
        "**\n",
        "\n",
        "JSON:\n",
        "'''\n",
        "\n",
        "print(\"***** Prompt         :\")\n",
        "print(geval_phase1_prompt)\n",
        "\n",
        "answer_phase1 = llm_run(geval_phase1_prompt)\n",
        "\n",
        "print(\"***** Answer         :\")\n",
        "print(answer_phase1)\n"
      ],
      "metadata": {
        "id": "fo-nE5KcAVXP",
        "outputId": "787d5237-2f55-43c5-87ca-5aad51276f0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Prompt         :\n",
            "Given an evaluation criteria which outlines how you should judge the Actual Output, generate\n",
            "3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output in\n",
            "relation to one another.\n",
            "\n",
            "Evaluation Criteria:\n",
            "Grade the english grammar and syntax\n",
            "\n",
            "**\n",
            "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or\n",
            "explanation is needed.\n",
            "Example JSON:\n",
            "{\n",
            "    \"steps\": <list_of_strings>\n",
            "}\n",
            "**\n",
            "\n",
            "JSON:\n",
            "\n",
            "***** Answer         :\n",
            "{\n",
            "    \"steps\": [\n",
            "        \"Assess the overall grammatical correctness of each output, noting any errors in sentence structure, punctuation, or verb tense consistency.\",\n",
            "        \"Evaluate the clarity and coherence of the writing, ensuring that ideas are logically organized and effectively communicated.\",\n",
            "        \"Compare the complexity and variety of the vocabulary used, determining if higher-level language is appropriately implemented.\",\n",
            "        \"Check for consistency in tone and style, ensuring that the output maintains an appropriate voice for the intended audience.\"\n",
            "    ]\n",
            "}\n",
            "CPU times: user 112 ms, sys: 6 ms, total: 118 ms\n",
            "Wall time: 1.41 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_answer_step1=json.loads(answer_phase1)\n",
        "steps=\"\\n\".join(f\"{index+1}. {step}\" for index, step in enumerate(json_answer_step1['steps']))\n",
        "print(steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwct7DYOdQp",
        "outputId": "01c75246-87b4-4f6a-b51c-d19f1c66f1cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Assess the overall grammatical correctness of each output, noting any errors in sentence structure, punctuation, or verb tense consistency.\n",
            "2. Evaluate the clarity and coherence of the writing, ensuring that ideas are logically organized and effectively communicated.\n",
            "3. Compare the complexity and variety of the vocabulary used, determining if higher-level language is appropriately implemented.\n",
            "4. Check for consistency in tone and style, ensuring that the output maintains an appropriate voice for the intended audience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G-Eval Phase 2: evaluate the llm_output using the generated steps"
      ],
      "metadata": {
        "id": "WuObfnA4HNF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input=\"Why do you dislike writing texts ?\"\n",
        "llm_output=\"Witing texts is painful, caus im making mitakes.\""
      ],
      "metadata": {
        "id": "L34HVzfyXRC9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geval_phase2_prompt = f'''\n",
        "Given the evaluation steps, return a JSON with two keys:\n",
        "1) a `score` key ranging from 0 - 10, with 10 being that it follows the criteria outlined in the steps and 0 being that it does not, and\n",
        "2) a `reason` key, a reason for the given score, but DO NOT QUOTE THE SCORE in your reason.\n",
        "Please mention specific information from Actual Output and Input in your reason, but be very concise with it!\n",
        "\n",
        "Evaluation Steps:\n",
        "{steps}\n",
        "\n",
        "Actual Output:\n",
        "{llm_output}\n",
        "\n",
        "Input:\n",
        "{llm_input}\n",
        "\n",
        "\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"score\" and \"reason\" key. No words or explanation is needed.\n",
        "\n",
        "Example JSON:\n",
        "{{\n",
        "    \"score\": 0,\n",
        "    \"reason\": \"The text does not follow the evaluation steps provided.\"\n",
        "}}\n",
        "**\n",
        "\n",
        "JSON:\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "QjYjN-9oH6VG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Prompt         :\")\n",
        "print(geval_phase2_prompt)\n",
        "\n",
        "answer_phase2=llm_run(geval_phase2_prompt)\n",
        "print(\"***** Answer         :\")\n",
        "print(answer_phase2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pla-jaYNPxgt",
        "outputId": "8c274955-62df-4c69-e8e4-703e96ff2e3e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Prompt         :\n",
            "\n",
            "Given the evaluation steps, return a JSON with two keys:\n",
            "1) a `score` key ranging from 0 - 10, with 10 being that it follows the criteria outlined in the steps and 0 being that it does not, and\n",
            "2) a `reason` key, a reason for the given score, but DO NOT QUOTE THE SCORE in your reason.\n",
            "Please mention specific information from Actual Output and Input in your reason, but be very concise with it!\n",
            "\n",
            "Evaluation Steps:\n",
            "1. Assess the overall grammatical correctness of each output, noting any errors in sentence structure, punctuation, or verb tense consistency.\n",
            "2. Evaluate the clarity and coherence of the writing, ensuring that ideas are logically organized and effectively communicated.\n",
            "3. Compare the complexity and variety of the vocabulary used, determining if higher-level language is appropriately implemented.\n",
            "4. Check for consistency in tone and style, ensuring that the output maintains an appropriate voice for the intended audience.\n",
            "\n",
            "Actual Output:\n",
            "Witing texts is painful, caus im making mitakes.\n",
            "\n",
            "Input:\n",
            "Why do you dislike writing texts ?\n",
            "\n",
            "\n",
            "\n",
            "**\n",
            "IMPORTANT: Please make sure to only return in JSON format, with the \"score\" and \"reason\" key. No words or explanation is needed.\n",
            "\n",
            "Example JSON:\n",
            "{\n",
            "    \"score\": 0,\n",
            "    \"reason\": \"The text does not follow the evaluation steps provided.\"\n",
            "}\n",
            "**\n",
            "\n",
            "JSON:\n",
            "\n",
            "\n",
            "***** Answer         :\n",
            "{\n",
            "    \"score\": 2,\n",
            "    \"reason\": \"The output contains grammatical errors such as 'Witing' instead of 'Writing' and 'caus' instead of 'because', which detracts from its clarity and coherence.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G-Eval Implementation by DeepEval"
      ],
      "metadata": {
        "id": "cu3nSLWzG_JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little adapter to connect our **llm_run** function with the deepEval library (and do some logging)\n",
        "\n",
        "see: https://docs.confident-ai.com/docs/guides-using-custom-llms"
      ],
      "metadata": {
        "id": "Z9fEWGsY6v18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**geval_run** calls deepEval's implementation passing our criteria, llm_input and llm_output"
      ],
      "metadata": {
        "id": "8hLIk5Zzp7jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import deepeval\n",
        "import deepeval.metrics\n",
        "import deepeval.test_case\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "def geval_run(name, criteria, input, output):\n",
        "    result = deepeval.evaluate(\n",
        "\n",
        "        test_cases=[deepeval.test_case.LLMTestCase(input=input, actual_output=output )],\n",
        "\n",
        "        metrics=[ GEval(\n",
        "              name=name,\n",
        "              criteria=criteria,\n",
        "              evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT,LLMTestCaseParams.INPUT],\n",
        "              model=None # deepEval defaults to openAI\n",
        "        )]\n",
        "    )\n",
        "    return result"
      ],
      "metadata": {
        "id": "2x3hftSQaea-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_output=\"\"\n",
        "r = geval_run(\"Language\", \"Grade the english grammar and syntax.\", llm_input, llm_output )"
      ],
      "metadata": {
        "id": "s-eQQ9sYcCKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539,
          "referenced_widgets": [
            "1920283f3ca5492c9c73e313f4449faf",
            "61729ae1f68d4cd49288729b837838ec"
          ]
        },
        "outputId": "16b323b4-adc2-478e-960e-971cc5a55cad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1920283f3ca5492c9c73e313f4449faf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Language (GEval) (score: 0.2238696496947669, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: Numerous grammatical errors in Actual Output including 'Witing' instead of 'Writing', 'caus' instead of 'because', and 'mitakes' instead of 'mistakes'. Poor punctuation and capitalization affect readability., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Why do you dislike writing texts ?\n",
            "  - actual output: Witing texts is painful, caus im making mitakes.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Language (GEval): 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "🎉 Tests finished ✅! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉 Tests finished ✅! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrics_data(deep_eval_result):\n",
        "    for testcase in deep_eval_result:\n",
        "      print(\"input        :\", testcase.input)\n",
        "      print(\"actual_output:\", testcase.actual_output)\n",
        "      print()\n",
        "      for metric in testcase.metrics_data:\n",
        "        print(\"name         :\",metric.name)\n",
        "        print(\"score        :\",metric.score)\n",
        "        print(\"reason       :\",metric.reason)\n",
        "        print(\"model        :\",metric.evaluation_model)\n",
        "        print()\n",
        "      print(\"-----------\")"
      ],
      "metadata": {
        "id": "r-xjg5guXVmK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_metrics_data(r)\n",
        "r"
      ],
      "metadata": {
        "id": "D7R_yZbFtx7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f9b346-786f-46ea-bd09-1aa0365c2d49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input        : Why do you dislike writing texts ?\n",
            "actual_output: Witing texts is painful, caus im making mitakes.\n",
            "\n",
            "name         : Language (GEval)\n",
            "score        : 0.2238696496947669\n",
            "reason       : Numerous grammatical errors in Actual Output including 'Witing' instead of 'Writing', 'caus' instead of 'because', and 'mitakes' instead of 'mistakes'. Poor punctuation and capitalization affect readability.\n",
            "model        : gpt-4o\n",
            "\n",
            "-----------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TestResult(success=False, metrics_data=[MetricData(name='Language (GEval)', threshold=0.5, success=False, score=0.2238696496947669, reason=\"Numerous grammatical errors in Actual Output including 'Witing' instead of 'Writing', 'caus' instead of 'because', and 'mitakes' instead of 'mistakes'. Poor punctuation and capitalization affect readability.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00425, verbose_logs='Criteria:\\nGrade the english grammar and syntax. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the grammatical structure of Input and Actual Output, ensuring they follow standard English grammar rules.\",\\n    \"Evaluate the syntax in both the Input and Actual Output, checking for proper sentence construction and coherence.\",\\n    \"Identify and note any grammatical errors or inconsistencies in the Actual Output compared to the Input.\",\\n    \"Assess the use of punctuation, capitalization, and overall readability in both the Input and Actual Output.\"\\n]')], input='Why do you dislike writing texts ?', actual_output='Witing texts is painful, caus im making mitakes.', expected_output=None, context=None, retrieval_context=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_output)"
      ],
      "metadata": {
        "id": "u-sF6C_oM4cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c19174d-9f25-4af1-81d5-929ddf2e6334"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating multiple metrics: \"Conciseness\", AnswerRelevance, Toxicity,..."
      ],
      "metadata": {
        "id": "u91oPU40-0sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "check out some other metrics [https://docs.confident-ai.com/docs/metrics-introduction](https://docs.confident-ai.com/docs/metrics-introduction)"
      ],
      "metadata": {
        "id": "o8TGFB0gWPjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import AnswerRelevancyMetric, ToxicityMetric\n",
        "\n",
        "def metrics_run(input, output):\n",
        "    test_case = deepeval.test_case.LLMTestCase(\n",
        "        input=input,\n",
        "        actual_output=output\n",
        "      )\n",
        "\n",
        "    conciseness_metric = GEval(\n",
        "        name=\"Conciseness\",\n",
        "        criteria=\"Determine how concise the actual output is. Ignore the input.\",\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],\n",
        "    )\n",
        "\n",
        "    language_metric = GEval(\n",
        "        name=\"Language\",\n",
        "        criteria=\"Grade the english grammar and syntax. Ignore the input.\",\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],\n",
        "    )\n",
        "\n",
        "    metrics = [\n",
        "        conciseness_metric,\n",
        "        language_metric,\n",
        "        AnswerRelevancyMetric(),\n",
        "        # ToxicityMetric()\n",
        "    ]\n",
        "\n",
        "    eval_result = deepeval.evaluate(\n",
        "        test_cases=[test_case],\n",
        "        metrics=metrics,\n",
        "    )\n",
        "    return eval_result"
      ],
      "metadata": {
        "id": "4zCdsHgawTNW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input=\"What is a pipe used for ?\"\n",
        "llm_output_concise=\"A pipe is a tubular conduit used to transport fluids or sometimes solids.\"\n",
        "llm_output_inconcise=\"Pipes are beautiful, black and round. Because they are round they are very convenient and don't have any edges.\""
      ],
      "metadata": {
        "id": "RXbZChu78adp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_output=\"\"\n",
        "r=metrics_run(llm_input, llm_output_concise)\n",
        "print_metrics_data(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989,
          "referenced_widgets": [
            "749b5050f7314937a612f94aeb8da672",
            "cbeac9d65e46452ab244e0d7600674ab"
          ]
        },
        "id": "5u-IvdmpCKBI",
        "outputId": "fca25d34-cdf0-4620-e148-c0e8e88c4618"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "749b5050f7314937a612f94aeb8da672"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Conciseness (GEval) (score: 0.8523812670840514, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The output is succinct but includes 'or sometimes solids,' which is not strictly necessary given the input., error: None)\n",
            "  - ✅ Language (GEval) (score: 0.9974042643560711, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The text is grammatically correct, has proper syntax, and follows standard English sentence construction rules., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is fully relevant and directly addresses the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is a pipe used for ?\n",
            "  - actual output: A pipe is a tubular conduit used to transport fluids or sometimes solids.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Conciseness (GEval): 100.00% pass rate\n",
            "Language (GEval): 100.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "🎉 Tests finished ✅! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉 Tests finished ✅! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input        : What is a pipe used for ?\n",
            "actual_output: A pipe is a tubular conduit used to transport fluids or sometimes solids.\n",
            "\n",
            "name         : Conciseness (GEval)\n",
            "score        : 0.8523812670840514\n",
            "reason       : The output is succinct but includes 'or sometimes solids,' which is not strictly necessary given the input.\n",
            "model        : gpt-4o\n",
            "\n",
            "name         : Language (GEval)\n",
            "score        : 0.9974042643560711\n",
            "reason       : The text is grammatically correct, has proper syntax, and follows standard English sentence construction rules.\n",
            "model        : gpt-4o\n",
            "\n",
            "name         : Answer Relevancy\n",
            "score        : 1.0\n",
            "reason       : The score is 1.00 because the response is fully relevant and directly addresses the question without any irrelevant information. Great job!\n",
            "model        : gpt-4o\n",
            "\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(log_output)"
      ],
      "metadata": {
        "id": "lQaYwsdSF4oQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BO45ou9F-Qk4"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}