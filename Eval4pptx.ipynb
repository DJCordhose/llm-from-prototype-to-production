{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03100b21164f4603b219d6a282a4c27c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_770636a2efd248369b8a9e15436501e0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255mâ ¸\u001b[0m âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">â ¸</span> âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "770636a2efd248369b8a9e15436501e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd12742179da4d2daf6053318adac771": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_37ff79f49cee44f7b601b6a106be995f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mConciseness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDâ€¦\u001b[0m\nâœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDoneâ€¦\u001b[0m\nâœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDoneâ€¦\u001b[0m\nâœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mToxicity Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m              \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Conciseness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Dâ€¦</span>\nâœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Doneâ€¦</span>\nâœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Doneâ€¦</span>\nâœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Toxicity Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>              \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "37ff79f49cee44f7b601b6a106be995f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72afa7473c974b0b96c753a613b656c5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1bc99ab512e74f5fbc20b6ad14121b4b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mConciseness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDâ€¦\u001b[0m\nâœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDoneâ€¦\u001b[0m\nâœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m      \nâœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mToxicity Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDone! (3.35s)\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Conciseness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Dâ€¦</span>\nâœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Doneâ€¦</span>\nâœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>      \nâœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Toxicity Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Done! (3.35s)</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1bc99ab512e74f5fbc20b6ad14121b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/llm-from-prototype-to-production/blob/main/Eval4pptx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands on: LLM as a judge\n",
        "\n",
        "Goal\n",
        "* see how llm-as-a-judge works in principle\n",
        "* introduction to the G-Eval algorithm ([G-Eval on arxive ](https://arxiv.org/abs/2303.16634))\n",
        "* see how the algorithm uses prompts to generate the actual eval prompt\n",
        "* try out the [DeepEval library](https://docs.confident-ai.com/docs/guides-using-custom-llms)\n",
        "\n",
        "\n",
        "Requirements:\n",
        "* OpenAI api key"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SetUp : create an *llm_run* method calling OpenAI Gpt\n",
        "\n",
        "* define a simple **llm_run** method, that calls the gpt model\n",
        "* try out llm_run"
      ],
      "metadata": {
        "id": "HUWl-0x-524k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wnUuUSG68wlB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install openai\n",
        "!pip install deepeval==1.1.1 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "b798290e-428e-4cce-8324-58679fda9511"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.44.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.44.1-py3-none-any.whl (373 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.9/365.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m396.6/396.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m185.7/185.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCPU times: user 269 ms, sys: 43.3 ms, total: 313 ms\n",
            "Wall time: 36.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "h2KpOS917BeW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_run(messages):\n",
        "  if type(messages) == str:\n",
        "    messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "\n",
        "  client = OpenAI()\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=messages\n",
        "  )\n",
        "  result = completion.choices[0].message.content\n",
        "  return result"
      ],
      "metadata": {
        "id": "jlJxNTGs6xM3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out our model:"
      ],
      "metadata": {
        "id": "8b-8riV2Vf_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_run(\"who are you ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nf93aT-JVPol",
        "outputId": "656e735b-2709-4f2b-e5f9-cbaa9df69633"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm an artificial intelligence called Assistant, created by OpenAI. I'm here to help you with any questions or information you might need. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-as-a-judge: in principle"
      ],
      "metadata": {
        "id": "z0jYXr3F9pem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output=\"Witing texts is painful, caus im making mitakes.\""
      ],
      "metadata": {
        "id": "Zh74JhHgQOMl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_eval_prompt = f'''\n",
        "You are an expert on english language, grading a students text with scores between 0 and 10.\n",
        "A text written in proper english, in a fluent style, containing no grammatical or syntax errors is graded 10.\n",
        "A text written in a different language or with spelling errors gets a low score.\n",
        "Also give a detailed explanation why the score was chosen.\n",
        "Do not repeat the students text in your explanation.\n",
        "\n",
        "Always answer in the following json format:\n",
        "{{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"some reason\"\n",
        "}}\n",
        "\n",
        "Examples\n",
        "1. Student Text: Pipes are cylindrical conduits used to transport fluids or gases, typically made of materials like metal, plastic, or concrete.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"The text is written in english and does not contain any syntactical or grammatical erros\"\n",
        "  }}\n",
        "2. Student Text: Zwischen Neonlichtern und StraÃŸenlÃ¤rm trÃ¤um ich leise von Freiheit.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 2,\n",
        "    \"reason\": \"The text is written in german and not in english.\"\n",
        "  }}\n",
        "\n",
        "Student Text: {llm_output}\n",
        "Answer:\n",
        "'''"
      ],
      "metadata": {
        "id": "1fUQwWQd8W2D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_run(simple_eval_prompt)"
      ],
      "metadata": {
        "id": "90PhBKcH8Wu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b17f93d1-b64b-4828-eac9-62c580450e62"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"score\": 3,\\n    \"reason\": \"The text contains multiple spelling errors and informal contractions: \\'Witing\\' should be \\'Writing\\', \\'caus\\' should be \\'because\\', and \\'im\\' should be \\'I\\'m\\'. It is understandable but not well-written.\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llm-as-a-judge: G-Eval in principal"
      ],
      "metadata": {
        "id": "xtY98fXd6gl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Idea:** given just a criteria use an llm to generate a detailed evaluation prompt.\n",
        "\n",
        "https://arxiv.org/pdf/2303.16634"
      ],
      "metadata": {
        "id": "BwT8zKTlWg05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G-Eval Phase 1: generate evaluation steps based on the criteria"
      ],
      "metadata": {
        "id": "p4swZQuDG5GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria=\"Grade the english grammar and syntax\""
      ],
      "metadata": {
        "id": "fjD209UWH0Kw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geval_phase1_prompt = f'''Given an evaluation criteria which outlines how you should judge the Actual Output, generate\n",
        "3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output in\n",
        "relation to one another.\n",
        "\n",
        "Evaluation Criteria:\n",
        "{criteria}\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or\n",
        "explanation is needed.\n",
        "\n",
        "Example JSON:\n",
        "{{\n",
        "    \"steps\": <list_of_strings>\n",
        "}}\n",
        "**\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "answer_phase1 = llm_run(geval_phase1_prompt)\n",
        "print(answer_phase1)"
      ],
      "metadata": {
        "id": "fo-nE5KcAVXP",
        "outputId": "813be7f0-3b63-4753-c3f6-4119a47dd30c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"steps\": [\n",
            "        \"Check for grammatical errors such as subject-verb agreement, verb forms, and correct usage of articles and prepositions.\",\n",
            "        \"Evaluate sentence structure for clarity, coherence, and variety in sentence length and type.\",\n",
            "        \"Assess the punctuation to ensure it enhances readability and accurately conveys the intended message.\",\n",
            "        \"Verify the proper use of capitalization, spelling accuracy, and correct syntax throughout the text.\"\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_answer_step1=json.loads(answer_phase1)\n",
        "steps=\"\\n\".join(f\"{index+1}. {step}\" for index, step in enumerate(json_answer_step1['steps']))\n",
        "print(steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwct7DYOdQp",
        "outputId": "de10be95-162c-433a-faa0-e16834ce015c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Check for grammatical errors such as subject-verb agreement, verb forms, and correct usage of articles and prepositions.\n",
            "2. Evaluate sentence structure for clarity, coherence, and variety in sentence length and type.\n",
            "3. Assess the punctuation to ensure it enhances readability and accurately conveys the intended message.\n",
            "4. Verify the proper use of capitalization, spelling accuracy, and correct syntax throughout the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G-Eval Phase 2: evaluate the llm_output using the generated steps"
      ],
      "metadata": {
        "id": "WuObfnA4HNF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input=\"Why do you dislike writing texts ?\"\n",
        "llm_output=\"Witing texts is painful, caus im making mitakes.\""
      ],
      "metadata": {
        "id": "L34HVzfyXRC9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geval_phase2_prompt = f'''\n",
        "Given the evaluation steps, return a JSON with two keys:\n",
        "1) a `score` key ranging from 0 - 10, with 10 being that it follows the criteria outlined in the steps and 0 being that it does not, and\n",
        "2) a `reason` key, a reason for the given score, but DO NOT QUOTE THE SCORE in your reason.\n",
        "Please mention specific information from Actual Output and Input in your reason, but be very concise with it!\n",
        "\n",
        "Evaluation Steps:\n",
        "{steps}\n",
        "\n",
        "Actual Output:\n",
        "{llm_output}\n",
        "\n",
        "Input:\n",
        "{llm_input}\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"score\" and \"reason\" key. No words or explanation is needed.\n",
        "\n",
        "Example JSON:\n",
        "{{\n",
        "    \"score\": 0,\n",
        "    \"reason\": \"The text does not follow the evaluation steps provided.\"\n",
        "}}\n",
        "**\n",
        "\n",
        "JSON:\n",
        "'''"
      ],
      "metadata": {
        "id": "QjYjN-9oH6VG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_phase2=llm_run(geval_phase2_prompt)\n",
        "print(answer_phase2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pla-jaYNPxgt",
        "outputId": "9d8d39f9-ca93-4983-da98-5c60eca7e04f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "    \"score\": 2,\n",
            "    \"reason\": \"Multiple grammatical errors such as 'witing' and 'mitakes'; improper sentence structure and lack of clarity; poor punctuation; inconsistent capitalization and spelling mistakes present.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G-Eval Implementation by DeepEval"
      ],
      "metadata": {
        "id": "cu3nSLWzG_JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**geval_run** calls deepEval's implementation passing our criteria, llm_input and llm_output"
      ],
      "metadata": {
        "id": "8hLIk5Zzp7jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import deepeval\n",
        "import deepeval.metrics\n",
        "import deepeval.test_case\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "def geval_run(name, criteria, input, output):\n",
        "    return deepeval.evaluate(\n",
        "        test_cases=[deepeval.test_case.LLMTestCase(input=input, actual_output=output )],\n",
        "        metrics=[\n",
        "            GEval( criteria=criteria,\n",
        "                   evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT,LLMTestCaseParams.INPUT],\n",
        "                   model=None, # deepEval defaults to openAI\n",
        "                   name=name\n",
        "        )]\n",
        "    )"
      ],
      "metadata": {
        "id": "2x3hftSQaea-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_output=\"\"\n",
        "r = geval_run(\"Language\", \"Grade the english grammar and syntax.\", llm_input, llm_output )"
      ],
      "metadata": {
        "id": "s-eQQ9sYcCKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539,
          "referenced_widgets": [
            "03100b21164f4603b219d6a282a4c27c",
            "770636a2efd248369b8a9e15436501e0"
          ]
        },
        "outputId": "a6dec087-8d46-4276-955f-3dd08dd893c6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03100b21164f4603b219d6a282a4c27c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - âŒ Language (GEval) (score: 0.1922801796419053, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The Actual Output has multiple grammatical errors such as 'Witing' and 'caus im making mitakes', which do not align with the Input's correct grammar. The syntax structure is also poor compared to the Input, and there are inconsistencies in tense and subject-verb agreement., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Why do you dislike writing texts ?\n",
            "  - actual output: Witing texts is painful, caus im making mitakes.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Language (GEval): 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸ‰ Tests finished âœ…! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. â€¼ï¸ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ‰ Tests finished âœ…! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. â€¼ï¸ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating multiple metrics: \"Conciseness\", AnswerRelevance, Toxicity,..."
      ],
      "metadata": {
        "id": "u91oPU40-0sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "check out some other metrics [https://docs.confident-ai.com/docs/metrics-introduction](https://docs.confident-ai.com/docs/metrics-introduction)"
      ],
      "metadata": {
        "id": "o8TGFB0gWPjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import AnswerRelevancyMetric, ToxicityMetric\n",
        "\n",
        "def metrics_run(input, output):\n",
        "    test_case = deepeval.test_case.LLMTestCase(\n",
        "        input=input,\n",
        "        actual_output=output\n",
        "      )\n",
        "\n",
        "    conciseness_metric = GEval(\n",
        "        name=\"Conciseness\",\n",
        "        criteria=\"Determine how concise the actual output is. Ignore the input.\",\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],\n",
        "    )\n",
        "\n",
        "    language_metric = GEval(\n",
        "        name=\"Language\",\n",
        "        criteria=\"Grade the english grammar and syntax. Ignore the input.\",\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],\n",
        "    )\n",
        "\n",
        "    metrics = [\n",
        "        conciseness_metric,\n",
        "        language_metric,\n",
        "        AnswerRelevancyMetric(),\n",
        "        ToxicityMetric()\n",
        "    ]\n",
        "\n",
        "    eval_result = deepeval.evaluate(\n",
        "        test_cases=[test_case],\n",
        "        metrics=metrics,\n",
        "    )\n",
        "    return eval_result"
      ],
      "metadata": {
        "id": "4zCdsHgawTNW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r=metrics_run(llm_input, llm_output)"
      ],
      "metadata": {
        "id": "uJa4l6IOyJCd",
        "outputId": "64ecc1cd-c014-4028-a917-0704c1b646f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703,
          "referenced_widgets": [
            "cd12742179da4d2daf6053318adac771",
            "37ff79f49cee44f7b601b6a106be995f"
          ]
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd12742179da4d2daf6053318adac771"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - âŒ Conciseness (GEval) (score: 0.44643985836491035, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output conveys the message but contains spelling errors and redundancies like 'caus' and 'mitakes' which should be corrected for clarity., error: None)\n",
            "  - âŒ Language (GEval) (score: 0.22637047689837106, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: There are punctuation and capitalization errors, such as 'Witing' and 'caus im'. The sentence structure is mostly understandable but contains grammatical errors like 'caus im' and 'mitakes'. Overall fluency and readability are low due to these issues., error: None)\n",
            "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and addresses the input directly with no irrelevant statements. Great job!, error: None)\n",
            "  - âœ… Toxicity (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the output is entirely free of toxic content., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Why do you dislike writing texts ?\n",
            "  - actual output: Witing texts is painful, caus im making mitakes.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Conciseness (GEval): 0.00% pass rate\n",
            "Language (GEval): 0.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "Toxicity: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸ‰ Tests finished âœ…! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. â€¼ï¸ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ‰ Tests finished âœ…! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. â€¼ï¸ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input=\"What is a pipe used for ?\"\n",
        "llm_output_concise=\"A pipe is a tubular conduit used to transport fluids or sometimes solids.\"\n",
        "llm_output_inconcise=\"Pipes are beautiful, black and round. Because they are round they are very convenient and don't have any edges.\""
      ],
      "metadata": {
        "id": "RXbZChu78adp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r=metrics_run(llm_input, llm_output_inconcise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703,
          "referenced_widgets": [
            "72afa7473c974b0b96c753a613b656c5",
            "1bc99ab512e74f5fbc20b6ad14121b4b"
          ]
        },
        "id": "5u-IvdmpCKBI",
        "outputId": "508566cc-b4df-4413-e16d-3cf819573591"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72afa7473c974b0b96c753a613b656c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - âŒ Conciseness (GEval) (score: 0.14139483425727736, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output does not answer the input question about the use of a pipe and contains superfluous details about its shape and color., error: None)\n",
            "  - âœ… Language (GEval) (score: 0.7120739590084814, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The Actual Output mostly follows the criteria, but the sentence 'Because they are round they are very convenient and don't have any edges.' could be clearer with better punctuation (e.g., a comma after 'round')., error: None)\n",
            "  - âŒ Answer Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the statements provided discuss the appearance and shape of pipes rather than their use, which does not address the actual question., error: None)\n",
            "  - âœ… Toxicity (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output contains no toxic elements and is entirely appropriate., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is a pipe used for ?\n",
            "  - actual output: Pipes are beautiful, black and round. Because they are round they are very convenient and don't have any edges.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Conciseness (GEval): 0.00% pass rate\n",
            "Language (GEval): 100.00% pass rate\n",
            "Answer Relevancy: 0.00% pass rate\n",
            "Toxicity: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸ‰ Tests finished âœ…! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. â€¼ï¸ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ‰ Tests finished âœ…! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. â€¼ï¸ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BO45ou9F-Qk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}